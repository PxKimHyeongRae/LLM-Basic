# μ „κ΄‘ν λ©”μ‹μ§€ μƒμ„± AI - ν„μ¬ μƒν™© μ”μ•½

> **μ¤‘μ”**: μμ„Έν• μ»¨ν…μ¤νΈλ” `CLAUDE.md` μ°Έκ³ 

## π― ν”„λ΅μ νΈ λ©ν‘

**IoT μ„Όμ„ λ°μ΄ν„° β†’ κ³µμ› μ „κ΄‘ν λ©”μ‹μ§€ μλ™ μƒμ„± (μμ AI)**

- β… μ¤ν”„λΌμΈ ν™κ²½ λ€λΉ„ (λ΅μ»¬ λ¨λΈ)
- β… AI μ‹λ²” μ‚¬μ—… (κ·μΉ™ κΈ°λ° λ¶κ°€)
- β… ν•κµ­μ–΄ μµμ ν™” (KORMo-10B)

## π“ ν„μ¬ μƒνƒ

### λ¬Έμ μ 
```
μ…λ ¥: yesterday_temp=35, today_temp=25

ν„μ¬ μ¶λ ¥:
"****\n</task>\nμ–΄μ λ³΄λ‹¤ 10λ„ λ–¨μ–΄μ Έμ„ μ€μ€ν•λ‹ μ·μ„ λ‘κ»κ² μ…μΌμ„Έμ”..."
(λ…Όλ¦¬ μ¤λ¥: 25λ„λ” λ”°λ»ν•λ° "μ€μ€ν•λ‹¤", "μ· λ‘κ»κ²")

μ›ν•λ” μ¶λ ¥:
"μ–΄μ λ³΄λ‹¤ 10λ„ λ‚®μ•„μ Έ μ‹μ›ν•΄μ΅μµλ‹λ‹¤. κ³µμ› μ‚°μ±… μµκ³ μμ”."
```

### μ™„λ£λ μ‘μ—…
- β… κµ¬μ΅°ν™”λ ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§
- β… Stop sequences μ μ© (μ£Όμ„μ²λ¦¬)
- β… ν›„μ²λ¦¬ λ΅μ§ κµ¬ν„ (μ£Όμ„μ²λ¦¬)
- β… **LLMμ΄ μ§μ ‘ μ‘μ„±ν• μμ—°μ¤λ¬μ΄ ν•™μµ λ°μ΄ν„° (120κ°)** β† **μµμ‹ **

### λ―Έμ™„λ£ μ‘μ—…
- β³ **λ°μ΄ν„° μƒμ„± μ‹¤ν–‰** β† μ§€κΈ μ—¬κΈ°
- β³ νμΈνλ‹ μ‹¤ν–‰
- β³ μ„±λ¥ ν‰κ°€
- β³ ν›„μ²λ¦¬ ν™μ„±ν™” μ—¬λ¶€ κ²°μ •

---

## π€ λ‹¤μ μ‹¤ν–‰ λ‹¨κ³„

### 1λ‹¨κ³„: ν•™μµ λ°μ΄ν„° μƒμ„± (5λ¶„)
```bash
python scripts/generate_more_training_data.py
```

**μƒμ„± κ²°κ³Ό**:
- `data/train_large.jsonl` (~270κ°)
- `data/validation_large.jsonl` (~30κ°)

### 2λ‹¨κ³„: νμΈνλ‹ μ‹¤ν–‰ (1-2μ‹κ°„)
```bash
python scripts/finetune_model_v2.py
```

**μ„¤μ •**:
- ν•™μµ λ°μ΄ν„°: 300κ°
- Epoch: 5
- LoRA rank: 32
- GPU λ©”λ¨λ¦¬: ~6GB

**μ €μ¥ μ„μΉ**: `./finetuned_model_v2/`

### 3λ‹¨κ³„: νμΈνλ‹ λ¨λΈ μ μ©

`.env` νμΌ μμ •:
```bash
USE_FINETUNED=true
ADAPTER_PATH=./finetuned_model_v2
```

μ„λ²„ μ¬μ‹μ‘:
```bash
python model_server.py
```

### 4λ‹¨κ³„: μ„±λ¥ ν…μ¤νΈ
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "μ–΄μ  10λ„, μ¤λ 5λ„",
    "max_new_tokens": 50,
    "temperature": 0.7
  }'
```

---

## π“ μμƒ κ²°κ³Ό

### Before νμΈνλ‹
```
β μ¥ν™©ν• μ¶λ ¥ (λ§ν¬λ‹¤μ΄, μ„¤λ…, λ€μ•)
β ν•μ‹ λ¶μΌμΉ
β μ •λ³΄ λ„λ½
```

### After νμΈνλ‹ (300κ° λ°μ΄ν„°)
```
β… κ°„κ²°ν• ν• λ¬Έμ¥
β… μΌκ΄€λ ν•μ‹
β… μ¨λ„ μ •λ³΄ λ…μ‹
β… μ μ ν• μ΅°μ–Έ
```

---

## π“ μ£Όμ” νμΌ

| νμΌ | μ„¤λ… |
|------|------|
| `CLAUDE.md` | **μ „μ²΄ μ»¨ν…μ¤νΈ λ° μƒμ„Έ κ°€μ΄λ“** |
| `model_server.py` | λ¨λΈ μ„λ²„ (Stop sequences ν¬ν•¨) |
| `src/generator/prompt_templates.py` | κµ¬μ΅°ν™”λ ν”„λ΅¬ν”„νΈ |
| `scripts/generate_more_training_data.py` | **μ‹¤ν–‰ ν•„μ”: λ°μ΄ν„° μƒμ„±** |
| `scripts/finetune_model_v2.py` | **μ‹¤ν–‰ ν•„μ”: νμΈνλ‹** |
| `.env` | ν™κ²½ μ„¤μ • |

---

## β΅ Quick Start

```bash
# 1. λ°μ΄ν„° μƒμ„±
python scripts/generate_more_training_data.py

# 2. νμΈνλ‹
python scripts/finetune_model_v2.py

# 3. μ„¤μ • λ³€κ²½ (.env)
# USE_FINETUNED=true
# ADAPTER_PATH=./finetuned_model_v2

# 4. μ„λ²„ μ¬μ‹μ‘
python model_server.py
```

---

## π” λ¬Έμ  ν•΄κ²°

### Q: νμΈνλ‹ μ¤‘ λ©”λ¨λ¦¬ λ¶€μ΅±
```python
# scripts/finetune_model_v2.py μμ •
per_device_train_batch_size=1  # 2 β†’ 1
gradient_accumulation_steps=16  # 8 β†’ 16
```

### Q: μ„±λ¥μ΄ μ—¬μ „ν λ¶€μ΅±
1. Epoch μ¦κ°€ (5 β†’ 10)
2. λ°μ΄ν„° μ¶”κ°€ (300 β†’ 500κ°)
3. DPO νμΈνλ‹ κ³ λ ¤ (`scripts/finetune_dpo.py`)

### Q: ν›„μ²λ¦¬κ°€ ν•„μ”ν•μ§€?
- νμΈνλ‹ ν›„ ν‰κ°€
- ν’μ§ μΆ‹μΌλ©΄ β†’ λ¶ν•„μ”
- ν’μ§ λ¶€μ΅± β†’ `model_server.py:332` μ£Όμ„ ν•΄μ 

---

## π“ μ„±κ³µ μ§€ν‘

- β… μ¶λ ¥ ν•μ‹: ν• λ¬Έμ¥, λ§ν¬λ‹¤μ΄ μ—†μ
- β… μ •λ³΄ μ™„μ „μ„±: μ¨λ„ μ°¨μ΄ + μ΅°μ–Έ
- β… κΈΈμ΄: 40-70μ
- β… μΌκ΄€μ„±: λ™μΌ μ…λ ¥ β†’ μ μ‚¬ μ¶λ ¥
- β… μ‘λ‹µ μ‹κ°„: 15μ΄ μ΄λ‚΄

---

**λ§μ§€λ§‰ μ—…λ°μ΄νΈ**: 2025-11-05
**ν„μ¬ λ‹¨κ³„**: Phase 1 - Step 1 (λ°μ΄ν„° μƒμ„± λ€κΈ°)
