"""
DPO (Direct Preference Optimization) íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

ëª©í‘œ:
- Chosen/Rejected ìŒìœ¼ë¡œ ì„ í˜¸ë„ í•™ìŠµ
- ê·œì¹™ ì—†ì´ ëª¨ë¸ì´ ì¢‹ì€ ì¶œë ¥ íŒ¨í„´ í•™ìŠµ
- Output cleaning ë¶ˆí•„ìš”
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel
from trl import DPOTrainer, DPOConfig
from dotenv import load_dotenv

load_dotenv()


def main():
    # ì„¤ì •
    MODEL_NAME = os.getenv('MODEL_NAME', 'KORMo-Team/KORMo-10B-sft')
    BASE_MODEL_PATH = os.getenv('ADAPTER_PATH', './finetuned_model')  # ê¸°ì¡´ íŒŒì¸íŠœë‹ ëª¨ë¸
    OUTPUT_DIR = "./finetuned_model_dpo"
    DPO_DATA_FILE = "data/dpo_dataset.jsonl"

    print(f"ğŸš€ DPO í•™ìŠµ ì‹œì‘")
    print(f"   ë² ì´ìŠ¤ ëª¨ë¸: {MODEL_NAME}")
    print(f"   ê¸°ì¡´ ì–´ëŒ‘í„°: {BASE_MODEL_PATH}")
    print(f"   ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")

    # 1. ì–‘ìí™” ì„¤ì • (4bit)
    print("\n1ï¸âƒ£ ì–‘ìí™” ì„¤ì • (4bit)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\n2ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # 3. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print("\n3ï¸âƒ£ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )

    # 4. ê¸°ì¡´ íŒŒì¸íŠœë‹ ì–´ëŒ‘í„° ë¡œë“œ (ìˆë‹¤ë©´)
    if os.path.exists(BASE_MODEL_PATH):
        print(f"\n4ï¸âƒ£ ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ: {BASE_MODEL_PATH}")
        model = PeftModel.from_pretrained(model, BASE_MODEL_PATH)
        # DPOë¥¼ ìœ„í•´ merge & unload (ì„ íƒì )
        # model = model.merge_and_unload()
        print("   âœ“ ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"\n4ï¸âƒ£ ê¸°ì¡´ ì–´ëŒ‘í„° ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì§„í–‰")

    # 5. QLoRA ì¤€ë¹„
    print("\n5ï¸âƒ£ QLoRA ì„¤ì •...")
    model = prepare_model_for_kbit_training(model)

    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 6. DPO ë°ì´í„°ì…‹ ë¡œë“œ
    print("\n6ï¸âƒ£ DPO ë°ì´í„°ì…‹ ë¡œë“œ...")
    dataset = load_dataset('json', data_files=DPO_DATA_FILE, split='train')

    print(f"   ì´ ë°ì´í„°: {len(dataset)}ìŒ")

    # ìƒ˜í”Œ ì¶œë ¥
    print("\n   [ìƒ˜í”Œ]")
    sample = dataset[0]
    print(f"   í”„ë¡¬í”„íŠ¸: {sample['prompt'][:60]}...")
    print(f"   Chosen: {sample['chosen'][:60]}...")
    print(f"   Rejected: {sample['rejected'][:60]}...")

    # 7. í•™ìŠµ ì„¤ì •
    print("\n7ï¸âƒ£ DPO í•™ìŠµ ì„¤ì •...")
    training_args = DPOConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,  # DPOëŠ” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
        fp16=True,
        logging_steps=5,
        save_strategy="steps",
        save_steps=50,
        save_total_limit=2,
        warmup_steps=10,
        lr_scheduler_type="cosine",
        optim="paged_adamw_8bit",
        report_to="none",
        gradient_checkpointing=True,
        remove_unused_columns=False,  # DPOì— ì¤‘ìš”
    )

    # ëª¨ë¸ ì„¤ì •
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.use_cache = False

    # 8. DPO Trainer ì„¤ì •
    print("\n8ï¸âƒ£ DPO Trainer ì„¤ì •...")

    # ìµœì‹  TRL APIì— ë§ì¶° ë‹¨ìˆœí™” (beta, max_length ë“±ì€ ê¸°ë³¸ê°’ ì‚¬ìš©)
    dpo_trainer = DPOTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer,
    )

    # 9. í•™ìŠµ ì‹œì‘
    print("\n9ï¸âƒ£ DPO í•™ìŠµ ì‹œì‘...\n")
    dpo_trainer.train()

    # 10. ëª¨ë¸ ì €ì¥
    print("\nğŸ”Ÿ ëª¨ë¸ ì €ì¥...")
    dpo_trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"\nâœ… DPO í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 11. í•™ìŠµ í†µê³„ ì¶œë ¥
    print("\nğŸ“Š í•™ìŠµ í†µê³„:")
    if dpo_trainer.state.log_history:
        losses = [x for x in dpo_trainer.state.log_history if 'loss' in x]

        if losses:
            initial_loss = losses[0].get('loss', 0)
            final_loss = losses[-1].get('loss', 0)
            print(f"  - ì´ˆê¸° ì†ì‹¤: {initial_loss:.4f}")
            print(f"  - ìµœì¢… ì†ì‹¤: {final_loss:.4f}")

            if initial_loss > 0:
                improvement = ((initial_loss - final_loss) / initial_loss * 100)
                print(f"  - ì†ì‹¤ ê°œì„ : {improvement:.1f}%")

    print("\nğŸ¯ íš¨ê³¼:")
    print("  - íŠ¹ìˆ˜ í† í° ìë™ ì œê±° í•™ìŠµ")
    print("  - ì§ˆë¬¸ í˜•ì‹ ìë™ ì œê±° í•™ìŠµ")
    print("  - ê°„ê²°í•˜ê³  ê¹”ë”í•œ ì¶œë ¥ í•™ìŠµ")
    print("  - Output cleaning ê·œì¹™ ë¶ˆí•„ìš”!")

    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. .envì—ì„œ ADAPTER_PATHë¥¼ ./finetuned_model_dpoë¡œ ë³€ê²½")
    print("  2. ëª¨ë¸ ì„œë²„ ì¬ì‹œì‘")
    print("  3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰")


if __name__ == "__main__":
    main()


import requests

test_cases = [
  {"yesterday": 0, "today": 1, "expected": "1ë„ ì°¨ì´ - íŠ¹ìˆ˜í† í° ì—†ì´ ê¹”ë”í•˜ê²Œ"},
  {"yesterday": -10, "today": 10, "expected": "20ë„ ìƒìŠ¹ - ì§ˆë¬¸ ì—†ì´"},
  {"yesterday": 20, "today": 30, "expected": "10ë„ ìƒìŠ¹ - ê°„ê²°í•˜ê²Œ"},
  {"yesterday": 30, "today": 20, "expected": "10ë„ í•˜ê°• - ì˜¨ë„ ì°¨ì´ ëª…ì‹œ"},
]

for case in test_cases:
  prompt = f"ì–´ì œì˜ í‰ê· ì˜¨ë„ëŠ” {case['yesterday']}ë„ê³  ì˜¤ëŠ˜ì˜ í‰ê· ì˜¨ë„ëŠ” {case['today']}ë„ì•¼. " \
           f"ì´ëŸ° ê²½ìš°ì— ê³µì›ì„ ë°©ë¬¸í•˜ëŠ” ê³ ê°ë“¤ì—ê²Œ ì ì ˆí•˜ê²Œ ì „ë‹¬í•´ì¤„ ì „ê´‘íŒ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜"

  response = requests.post(
      "http://localhost:8000/generate",
      json={"prompt": prompt, "max_new_tokens": 100}
  )

  result = response.json()
  print(f"\n{'='*60}")
  print(f"ì…ë ¥: {case['yesterday']}ë„ â†’ {case['today']}ë„")
  print(f"ê¸°ëŒ€: {case['expected']}")
  print(f"ì¶œë ¥: {result['generated_text']}")
  print(f"{'='*60}")