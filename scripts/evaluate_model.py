"""
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ì›ë³¸ ëª¨ë¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""

import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from dotenv import load_dotenv

load_dotenv()


def load_finetuned_model(base_model_name: str, adapter_path: str):
    """
    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"ğŸ“¦ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # ì–‘ìí™” ì„¤ì •
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        bnb_8bit_compute_dtype=torch.float16,
    )

    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )

    # LoRA ì–´ëŒ‘í„° ì ìš©
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        adapter_path,
        trust_remote_code=True,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model, tokenizer


def generate_message(model, tokenizer, user_input: str, max_length: int = 150) -> str:
    """
    ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""ì•„ë˜ ì…ë ¥ì„ ê³µì› ì „ê´‘íŒì— í‘œì‹œí•  ë©”ì‹œì§€ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì…ë ¥: {user_input}
ì¶œë ¥:"""

    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # "ì¶œë ¥:" ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if "ì¶œë ¥:" in generated_text:
        result = generated_text.split("ì¶œë ¥:")[1].strip()
        # ë‹¤ìŒ ì¤„ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        result = result.split('\n')[0].strip()
        return result
    else:
        return generated_text


def evaluate_on_test_cases(model, tokenizer):
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    test_cases = [
        "ì˜¨ë„ 35ë„ í­ì—¼",
        "ìŠµë„ 85% ì°œí†µë”ìœ„",
        "ë³€ìœ„ê³„ ì´ìƒ ê°ì§€",
        "ë¯¸ì„¸ë¨¼ì§€ ë‚˜ì¨",
        "ì“°ë ˆê¸° ë¶„ë¦¬ìˆ˜ê±°",
        "ë°˜ë ¤ë™ë¬¼ ë™ë°˜ ì£¼ì˜",
        "ê²¨ìš¸ ë¹™íŒê¸¸ ì£¼ì˜",
        "ì—¬ë¦„ ë¬¼ ì„­ì·¨ ê¶Œì¥",
        "ì–´ë¦°ì´ ë†€ì´í„° ì•ˆì „",
        "ì•¼ê°„ ìš´ì˜ì‹œê°„ ì•ˆë‚´",
    ]

    print("\n" + "="*60)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€")
    print("="*60)

    for i, test_input in enumerate(test_cases, 1):
        output = generate_message(model, tokenizer, test_input)
        print(f"\n[{i}] ì…ë ¥: {test_input}")
        print(f"    ì¶œë ¥: {output}")
        print(f"    ê¸¸ì´: {len(output)}ì")


def evaluate_on_validation_data(model, tokenizer, val_file: str = "data/validation.jsonl"):
    """
    ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(val_file):
        print(f"\nâš ï¸  ê²€ì¦ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {val_file}")
        return

    with open(val_file, 'r', encoding='utf-8') as f:
        val_data = [json.loads(line) for line in f]

    print("\n" + "="*60)
    print("ğŸ“Š ê²€ì¦ ë°ì´í„° í‰ê°€ (5ê°œ ìƒ˜í”Œ)")
    print("="*60)

    for i, sample in enumerate(val_data[:5], 1):
        user_input = sample['input']
        expected_output = sample['output']
        generated_output = generate_message(model, tokenizer, user_input)

        print(f"\n[{i}] ì…ë ¥: {user_input}")
        print(f"    ì •ë‹µ: {expected_output}")
        print(f"    ìƒì„±: {generated_output}")
        print(f"    ê¸¸ì´: {len(generated_output)}ì")


def interactive_test(model, tokenizer):
    """
    ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    """
    print("\n" + "="*60)
    print("ğŸ’¬ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¢…ë£Œ: 'q' ì…ë ¥)")
    print("="*60)

    while True:
        user_input = input("\nì…ë ¥: ").strip()

        if user_input.lower() in ['q', 'quit', 'exit']:
            print("í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not user_input:
            continue

        output = generate_message(model, tokenizer, user_input)
        print(f"ì¶œë ¥: {output}")
        print(f"ê¸¸ì´: {len(output)}ì")


def main():
    BASE_MODEL = os.getenv('MODEL_NAME', 'KORMo-Team/KORMo-10B-sft')
    ADAPTER_PATH = "./finetuned_model"

    if not os.path.exists(ADAPTER_PATH):
        print(f"âŒ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {ADAPTER_PATH}")
        print("ë¨¼ì € finetune_model.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_finetuned_model(BASE_MODEL, ADAPTER_PATH)

    # í‰ê°€ ì‹¤í–‰
    evaluate_on_test_cases(model, tokenizer)
    evaluate_on_validation_data(model, tokenizer)

    # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)
    choice = input("\nëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if choice == 'y':
        interactive_test(model, tokenizer)


if __name__ == "__main__":
    main()
