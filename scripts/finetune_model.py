"""
QLoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
KORMo-10B-sft ëª¨ë¸ì„ ì „ê´‘íŒ ë©”ì‹œì§€ ìƒì„± íƒœìŠ¤í¬ì— ë§ê²Œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.
"""

import os
import json
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from dotenv import load_dotenv

load_dotenv()


def format_instruction(sample):
    """
    í•™ìŠµ ë°ì´í„°ë¥¼ instruction í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """
    instruction = f"""ì•„ë˜ ì…ë ¥ì„ ê³µì› ì „ê´‘íŒì— í‘œì‹œí•  ë©”ì‹œì§€ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì…ë ¥: {sample['input']}
ì¶œë ¥: {sample['output']}"""

    return instruction


def main():
    # ì„¤ì •
    MODEL_NAME = os.getenv('MODEL_NAME', 'KORMo-Team/KORMo-10B-sft')
    OUTPUT_DIR = "./finetuned_model"
    TRAIN_FILE = "data/train_merged.jsonl"  # ê¸°ì¡´ + ì˜¨ë„ ë¹„êµ ë°ì´í„° (401ê°œ)
    VAL_FILE = "data/validation.jsonl"

    print(f"ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘: {MODEL_NAME}")

    # 1. ì–‘ìí™” ì„¤ì • (4bit)
    print("\n1ï¸âƒ£ ì–‘ìí™” ì„¤ì • (4bit)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\n2ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # 3. ëª¨ë¸ ë¡œë“œ
    print("\n3ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )

    # 4. QLoRAë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
    print("\n4ï¸âƒ£ QLoRA ì„¤ì •...")
    model = prepare_model_for_kbit_training(model)

    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=16,  # LoRA rank
        lora_alpha=32,  # LoRA alpha
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],  # KORMoì˜ attention ë° MLP ë ˆì´ì–´
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print("\n5ï¸âƒ£ ë°ì´í„°ì…‹ ë¡œë“œ...")
    train_dataset = load_dataset('json', data_files=TRAIN_FILE, split='train')
    val_dataset = load_dataset('json', data_files=VAL_FILE, split='train')

    print(f"  - í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"  - ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

    # ë°ì´í„° í¬ë§·íŒ…
    def formatting_func(example):
        text = f"""ì•„ë˜ ì…ë ¥ì„ ê³µì› ì „ê´‘íŒì— í‘œì‹œí•  ë©”ì‹œì§€ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì…ë ¥: {example['input']}
ì¶œë ¥: {example['output']}"""
        return text

    # 6. í•™ìŠµ ì„¤ì •
    print("\n6ï¸âƒ£ í•™ìŠµ ì„¤ì •...")
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=1,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 1ë¡œ ì„¤ì •
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì¸ ë§Œí¼ ì¦ê°€
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,
        warmup_steps=50,
        lr_scheduler_type="cosine",
        optim="paged_adamw_8bit",
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    )

    # ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì„¤ì •
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.use_cache = False  # gradient checkpointingê³¼ ì¶©ëŒ ë°©ì§€

    # 7. Trainer ì„¤ì •
    print("\n7ï¸âƒ£ Trainer ì„¤ì •...")

    # ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
    def preprocess_function(examples):
        texts = []
        for inp, out in zip(examples['input'], examples['output']):
            text = f"""ì•„ë˜ ì…ë ¥ì„ ê³µì› ì „ê´‘íŒì— í‘œì‹œí•  ë©”ì‹œì§€ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì…ë ¥: {inp}
ì¶œë ¥: {out}"""
            texts.append(text)

        # í† í¬ë‚˜ì´ì§•
        model_inputs = tokenizer(
            texts,
            max_length=512,
            truncation=True,
            padding='max_length',
            return_tensors=None,
        )

        # labels ì„¤ì • (input_ids ë³µì‚¬)
        model_inputs["labels"] = model_inputs["input_ids"].copy()

        return model_inputs

    # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    train_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=train_dataset.column_names,
    )

    val_dataset = val_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=val_dataset.column_names,
    )

    from transformers import Trainer, DataCollatorForLanguageModeling

    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )

    # 8. í•™ìŠµ ì‹œì‘
    print("\n8ï¸âƒ£ í•™ìŠµ ì‹œì‘...\n")
    trainer.train()

    # 9. ëª¨ë¸ ì €ì¥
    print("\n9ï¸âƒ£ ëª¨ë¸ ì €ì¥...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 10. í•™ìŠµ í†µê³„ ì¶œë ¥
    print("\nğŸ“Š í•™ìŠµ í†µê³„:")
    if trainer.state.log_history:
        train_losses = [x for x in trainer.state.log_history if 'loss' in x]
        eval_losses = [x for x in trainer.state.log_history if 'eval_loss' in x]

        if train_losses:
            final_train_loss = train_losses[-1]['loss']
            print(f"  - ìµœì¢… í•™ìŠµ ì†ì‹¤: {final_train_loss:.4f}")

        if eval_losses:
            final_eval_loss = eval_losses[-1]['eval_loss']
            print(f"  - ìµœì¢… ê²€ì¦ ì†ì‹¤: {final_eval_loss:.4f}")
        else:
            print(f"  - ê²€ì¦ ì†ì‹¤: ë°ì´í„°ê°€ ì ì–´ ê²€ì¦ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if train_losses:
            initial_loss = train_losses[0].get('loss', 0)
            final_loss = train_losses[-1].get('loss', 0)
            improvement = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0
            print(f"  - ì†ì‹¤ ê°œì„ : {improvement:.1f}%")


if __name__ == "__main__":
    main()
