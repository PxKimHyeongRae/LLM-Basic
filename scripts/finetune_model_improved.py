"""
ê°œì„ ëœ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ì˜¨ë„ ë…¼ë¦¬ê°€ ê°œì„ ëœ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
"""

import os
import json
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from dotenv import load_dotenv

load_dotenv()


def main():
    # ì„¤ì •
    MODEL_NAME = os.getenv('MODEL_NAME', 'KORMo-Team/KORMo-10B-sft')
    OUTPUT_DIR = "./finetuned_model_improved"

    # ê°œì„ ëœ ë°ì´í„° ì‚¬ìš©
    TRAIN_FILE = "data/train_improved.jsonl"
    VAL_FILE = "data/validation_improved.jsonl"

    print("=" * 70)
    print(f"ğŸš€ ê°œì„ ëœ íŒŒì¸íŠœë‹ ì‹œì‘: {MODEL_NAME}")
    print("   ì˜¨ë„ ë…¼ë¦¬ê°€ ê°œì„ ëœ ë°ì´í„° ì‚¬ìš©")
    print(f"   í•™ìŠµ ë°ì´í„°: {TRAIN_FILE}")
    print("=" * 70)

    # 1. ì–‘ìí™” ì„¤ì • (4bit)
    print("\n1ï¸âƒ£ ì–‘ìí™” ì„¤ì • (4bit)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\n2ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # 3. ëª¨ë¸ ë¡œë“œ
    print("\n3ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )

    # 4. QLoRAë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
    print("\n4ï¸âƒ£ QLoRA ì„¤ì •...")
    model = prepare_model_for_kbit_training(model)

    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print("\n5ï¸âƒ£ ë°ì´í„°ì…‹ ë¡œë“œ...")
    train_dataset = load_dataset('json', data_files=TRAIN_FILE, split='train')
    val_dataset = load_dataset('json', data_files=VAL_FILE, split='train')

    print(f"  - í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"  - ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

    # ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
    print("\n[í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ]")
    print(train_dataset[0]['text'][:200] + "...")

    # 6. ë°ì´í„° ì „ì²˜ë¦¬
    print("\n6ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬...")

    def preprocess_function(examples):
        model_inputs = tokenizer(
            examples['text'],
            max_length=512,
            truncation=True,
            padding='max_length',
            return_tensors=None,
        )
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs

    train_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬"
    )

    val_dataset = val_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=val_dataset.column_names,
        desc="ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬"
    )

    # 7. í•™ìŠµ ì„¤ì •
    print("\n7ï¸âƒ£ í•™ìŠµ ì„¤ì •...")
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=5,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=3e-4,
        fp16=True,
        logging_steps=5,
        eval_strategy="steps",
        eval_steps=20,
        save_strategy="steps",
        save_steps=20,
        save_total_limit=3,
        warmup_steps=30,
        lr_scheduler_type="cosine",
        optim="paged_adamw_8bit",
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        gradient_checkpointing=True,
        logging_first_step=True,
    )

    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.use_cache = False

    # 8. Data Collator
    print("\n8ï¸âƒ£ Data Collator ì„¤ì •...")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    # 9. Trainer ì„¤ì •
    print("\n9ï¸âƒ£ Trainer ì„¤ì •...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )

    # 10. í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 70)
    print("ğŸ”¥ ê°œì„ ëœ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘!")
    print("=" * 70 + "\n")

    trainer.train()

    # 11. ëª¨ë¸ ì €ì¥
    print("\n" + "=" * 70)
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 12. í•™ìŠµ í†µê³„ ì¶œë ¥
    print("\nğŸ“Š í•™ìŠµ í†µê³„:")
    print("=" * 70)

    if trainer.state.log_history:
        train_losses = [x for x in trainer.state.log_history if 'loss' in x]
        eval_losses = [x for x in trainer.state.log_history if 'eval_loss' in x]

        if train_losses:
            initial_loss = train_losses[0].get('loss', 0)
            final_loss = train_losses[-1].get('loss', 0)
            print(f"  ì´ˆê¸° í•™ìŠµ ì†ì‹¤: {initial_loss:.4f}")
            print(f"  ìµœì¢… í•™ìŠµ ì†ì‹¤: {final_loss:.4f}")

            if initial_loss > 0:
                improvement = ((initial_loss - final_loss) / initial_loss * 100)
                print(f"  ì†ì‹¤ ê°œì„ ìœ¨: {improvement:.1f}%")

        if eval_losses:
            best_eval_loss = min([x['eval_loss'] for x in eval_losses])
            print(f"  ìµœê³  ê²€ì¦ ì†ì‹¤: {best_eval_loss:.4f}")

    print("\n" + "=" * 70)
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. .env íŒŒì¼ì—ì„œ ë‹¤ìŒ ì„¤ì • ë³€ê²½:")
    print(f"     USE_FINETUNED=true")
    print(f"     ADAPTER_PATH={OUTPUT_DIR}")
    print("  2. ëª¨ë¸ ì„œë²„ ì¬ì‹œì‘:")
    print("     python model_server.py")
    print("  3. í…ŒìŠ¤íŠ¸:")
    print("     curl -X POST http://localhost:8000/generate/temperature \\")
    print("       -d '{\"yesterday_temp\": 35, \"today_temp\": 25}'")
    print("=" * 70)


if __name__ == "__main__":
    main()
